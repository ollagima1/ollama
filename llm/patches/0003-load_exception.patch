From f2c78629f709f3ff21a6f8e171055b52331ecd2a Mon Sep 17 00:00:00 2001
From: Liu Yuan <namei.unix@gmail.com>
Date: Sun, 13 Oct 2024 01:40:48 +0800
Subject: [PATCH] 03-load_exception.diff

---
 src/llama.cpp | 24 +++++++++++++++---------
 1 file changed, 15 insertions(+), 9 deletions(-)

diff --git a/src/llama.cpp b/src/llama.cpp
index f68024f5..aa75e6d1 100644
--- a/src/llama.cpp
+++ b/src/llama.cpp
@@ -9123,7 +9123,7 @@ static int llama_model_load(const std::string & fname, llama_model & model, llam
         }
     } catch (const std::exception & err) {
         LLAMA_LOG_ERROR("%s: error loading model: %s\n", __func__, err.what());
-        return -1;
+        throw;
     }
 
     // loading time will be recalculate after the first eval, so
@@ -19210,16 +19210,22 @@ struct llama_model * llama_load_model_from_file(
         }
     }
 
-    int status = llama_model_load(path_model, *model, params);
-    GGML_ASSERT(status <= 0);
-    if (status < 0) {
-        if (status == -1) {
-            LLAMA_LOG_ERROR("%s: failed to load model\n", __func__);
-        } else if (status == -2) {
-            LLAMA_LOG_INFO("%s: cancelled model load\n", __func__);
+    try {
+        int status = llama_model_load(path_model, *model, params);
+        GGML_ASSERT(status <= 0);
+        if (status < 0) {
+            if (status == -1) {
+                LLAMA_LOG_ERROR("%s: failed to load model\n", __func__);
+            } else if (status == -2) {
+                LLAMA_LOG_INFO("%s: cancelled model load\n", __func__);
+            }
+            llama_free_model(model);
+            return nullptr;
         }
+    } catch (...) {
+        LLAMA_LOG_ERROR("%s: exception loading model\n", __func__);
         llama_free_model(model);
-        return nullptr;
+        throw;
     }
 
     return model;
-- 
2.34.1

